{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6357ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "409d2d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b4c7c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset/ds.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataset/ds.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     training_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining text:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset/ds.txt'"
     ]
    }
   ],
   "source": [
    "with open('Dataset/ds.txt', 'r') as file:\n",
    "    training_text = file.read()\n",
    "\n",
    "print(\"training text:\")\n",
    "print(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2f1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: ['\\n', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'x', 'y']\n",
      "Total unique characters: 24\n",
      "\n",
      "Character to index mapping:\n",
      "  '\n",
      "' -> 0\n",
      "  ' ' -> 1\n",
      "  'a' -> 2\n",
      "  'b' -> 3\n",
      "  'c' -> 4\n",
      "  'd' -> 5\n",
      "  'e' -> 6\n",
      "  'f' -> 7\n",
      "  'g' -> 8\n",
      "  'h' -> 9\n",
      "\n",
      "Vocabulary size: 24\n"
     ]
    }
   ],
   "source": [
    "# Get all unique characters from our text\n",
    "chars = sorted(list(set(training_text.lower())))\n",
    "print(f\"Unique characters: {chars}\")\n",
    "print(f\"Total unique characters: {len(chars)}\")\n",
    "\n",
    "# Create mappings between characters and numbers\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "print(\"\\nCharacter to index mapping:\")\n",
    "for char, idx in list(char_to_idx.items())[:10]:  # Show first 10\n",
    "    print(f\"  '{char}' -> {idx}\")\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a53983f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'hello'\n",
      "To indices: [9, 6, 12, 12, 15]\n",
      "Back to text: 'hello'\n"
     ]
    }
   ],
   "source": [
    "def text_to_indices(text):\n",
    "    \"\"\"Convert text string to list of numbers\"\"\"\n",
    "    return [char_to_idx[char] for char in text.lower()]\n",
    "\n",
    "def indices_to_text(indices):\n",
    "    \"\"\"Convert list of numbers back to text\"\"\"\n",
    "    return ''.join([idx_to_char[idx] for idx in indices])\n",
    "\n",
    "# Let's test our conversion\n",
    "test_text = \"hello\"\n",
    "test_indices = text_to_indices(test_text)\n",
    "converted_back = indices_to_text(test_indices)\n",
    "\n",
    "print(f\"Original: '{test_text}'\")\n",
    "print(f\"To indices: {test_indices}\")\n",
    "print(f\"Back to text: '{converted_back}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2acccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: torch.Size([236, 15])\n",
      "Targets shape: torch.Size([236])\n",
      "\n",
      "First training example:\n",
      "Input: '\n",
      "hello world ho'\n",
      "Target: 'w'\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(text_indices, seq_length=20):\n",
    "    \"\"\"\n",
    "    Create training examples where:\n",
    "    - Input: sequence of characters\n",
    "    - Output: next character in sequence\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    \n",
    "    # Slide a window through the text\n",
    "    for i in range(len(text_indices) - seq_length):\n",
    "        # Input sequence\n",
    "        seq = text_indices[i:i + seq_length]\n",
    "        # Target (next character after the sequence)\n",
    "        target = text_indices[i + seq_length]\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        next_chars.append(target)\n",
    "    \n",
    "    return torch.tensor(sequences), torch.tensor(next_chars)\n",
    "\n",
    "# Convert our text to numbers\n",
    "text_indices = text_to_indices(training_text)\n",
    "\n",
    "# Create training data\n",
    "seq_length = 15  # We'll use 15 characters to predict the 16th\n",
    "X, y = create_sequences(text_indices, seq_length)\n",
    "\n",
    "print(f\"Input sequences shape: {X.shape}\")  # (num_sequences, seq_length)\n",
    "print(f\"Targets shape: {y.shape}\")         # (num_sequences,)\n",
    "\n",
    "# Let's look at one example\n",
    "print(f\"\\nFirst training example:\")\n",
    "input_seq = X[0]\n",
    "target_char = y[0]\n",
    "print(f\"Input: '{indices_to_text(input_seq.tolist())}'\")\n",
    "print(f\"Target: '{idx_to_char[target_char.item()]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1d435d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created!\n",
      "Parameters: 204,312\n"
     ]
    }
   ],
   "source": [
    "class AutocompleteRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers=2):\n",
    "        super(AutocompleteRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Embedding layer: converts character indices to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # GRU layer: our RNN that remembers patterns\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Output layer: predicts which character comes next\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        \n",
    "        # Step 1: Convert character indices to vectors\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Step 2: Pass through GRU (the RNN part)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        # Step 3: Get the last output and predict next character\n",
    "        output = self.fc(output[:, -1, :])  # Use only the last output\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "# Let's create our model\n",
    "hidden_size = 128\n",
    "model = AutocompleteRNN(vocab_size, hidden_size).to(device)\n",
    "print(\"Model created!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65c0818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete(model, start_text, max_length=50, temperature=0.8, seq_length=15):\n",
    "    \"\"\"Generate autocomplete suggestions - IMPROVED VERSION\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert start text to indices\n",
    "    start_indices = text_to_indices(start_text)\n",
    "    \n",
    "    # Handle short inputs by padding\n",
    "    if len(start_indices) < seq_length:\n",
    "        # We can either pad or use what we have\n",
    "        # Let's use what we have but warn the user\n",
    "        print(f\"Warning: Input '{start_text}' is shorter than sequence length {seq_length}\")\n",
    "        # We'll just use the available characters\n",
    "        current_sequence = start_indices\n",
    "    else:\n",
    "        # Use the last seq_length characters\n",
    "        current_sequence = start_indices[-seq_length:]\n",
    "    \n",
    "    generated = start_indices.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Convert to tensor with correct shape: (batch_size=1, sequence_length)\n",
    "        current_seq = torch.tensor([current_sequence]).to(device)\n",
    "        print(f\"Starting with sequence: '{indices_to_text(current_sequence)}'\")\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            # Forward pass\n",
    "            output, _ = model(current_seq)\n",
    "            \n",
    "            # Apply temperature\n",
    "            output = output / temperature\n",
    "            \n",
    "            # Get probabilities\n",
    "            probabilities = torch.softmax(output, dim=-1).cpu().numpy()[0]\n",
    "            \n",
    "            # Sample next character\n",
    "            next_char_idx = np.random.choice(len(probabilities), p=probabilities)\n",
    "            generated.append(next_char_idx)\n",
    "            \n",
    "            # Update sequence (sliding window)\n",
    "            new_sequence = generated[-seq_length:]\n",
    "            current_seq = torch.tensor([new_sequence]).to(device)\n",
    "            \n",
    "            # Optional: stop if we generate a newline or similar\n",
    "            if idx_to_char[next_char_idx] == '\\n':\n",
    "                break\n",
    "    \n",
    "    final_text = indices_to_text(generated)\n",
    "    print(f\"Final result: '{final_text}'\")\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d209d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Fixed Autocomplete ===\n",
      "Input: 'hello' -> 'hellow are you doing toda'\n",
      "Input: 'mach' -> 'machine learning is fun '\n",
      "Input: 'neur' -> 'neuril networks can lear'\n",
      "Input: 'pyt' -> 'python programming is g'\n"
     ]
    }
   ],
   "source": [
    "def autocomplete_working(model, start_text, max_length=30, temperature=0.8):\n",
    "    \"\"\"Working autocomplete function\"\"\"\n",
    "    model.eval()\n",
    "    seq_length = 15\n",
    "    \n",
    "    # Convert to indices\n",
    "    start_indices = text_to_indices(start_text)\n",
    "    generated = start_indices.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Handle sequence length\n",
    "        if len(start_indices) < seq_length:\n",
    "            # Pad with zeros (0 is usually space or most common char)\n",
    "            current_sequence = [0] * (seq_length - len(start_indices)) + start_indices\n",
    "        else:\n",
    "            current_sequence = start_indices[-seq_length:]\n",
    "        \n",
    "        current_seq = torch.tensor([current_sequence]).to(device)\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            # Get prediction\n",
    "            output, _ = model(current_seq)\n",
    "            output = output / temperature\n",
    "            \n",
    "            # Convert to probabilities and sample\n",
    "            probabilities = torch.softmax(output, dim=-1).cpu().numpy()[0]\n",
    "            next_char_idx = np.random.choice(len(probabilities), p=probabilities)\n",
    "            \n",
    "            # Add to generated\n",
    "            generated.append(next_char_idx)\n",
    "            \n",
    "            # Update sequence (sliding window)\n",
    "            current_sequence = generated[-seq_length:]\n",
    "            current_seq = torch.tensor([current_sequence]).to(device)\n",
    "    \n",
    "    return indices_to_text(generated)\n",
    "\n",
    "# Test it!\n",
    "print(\"\\n=== Testing Fixed Autocomplete ===\")\n",
    "test_inputs = [\"hello\", \"mach\", \"neur\", \"pyt\"]\n",
    "\n",
    "for test_input in test_inputs:\n",
    "    completion = autocomplete_working(model, test_input, max_length=20, temperature=0.7)\n",
    "    print(f\"Input: '{test_input}' -> '{completion}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ === INTERACTIVE AUTOCOMPLETE (FIXED) ===\n",
      "Type some text and see what the model suggests!\n",
      "You: hell\u001b[94me aod you doing today\n",
      "this is \u001b[0m\n",
      "You: hello  ho\u001b[94mw are you doing today\n",
      "this is \u001b[0m\n",
      "You: who i\u001b[94ms a simple autocomplete system\u001b[0m\n",
      "Please type at least one character\n",
      "Please type at least one character\n",
      "Please type at least one character\n"
     ]
    }
   ],
   "source": [
    "def interactive_demo_fixed():\n",
    "    print(\"\\nðŸŽ¯ === INTERACTIVE AUTOCOMPLETE (FIXED) ===\")\n",
    "    print(\"Type some text and see what the model suggests!\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nStart typing: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        elif len(user_input) < 1:\n",
    "            print(\"Please type at least one character\")\n",
    "            continue\n",
    "        \n",
    "        completion = autocomplete_working(model, user_input, max_length=30, temperature=0.7)\n",
    "        \n",
    "        # Show the original input and the completion in different colors\n",
    "        original_part = completion[:len(user_input)]\n",
    "        new_part = completion[len(user_input):]\n",
    "        print(f\"You: {original_part}\\033[94m{new_part}\\033[0m\")\n",
    "\n",
    "# Run the fixed demo\n",
    "interactive_demo_fixed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
